{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install langchain langchain-openai langchain-openai langchain_chroma langchain-text-splitters langchain_community langchainhub",
   "id": "b808d2bb216d194e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ],
   "id": "48e00aec618539aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate"
   ],
   "id": "8eaddcb2f70cc742"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. 3개의 블로그 포스팅 본문을 Load: WebBaseLoader 활용\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "loader = WebBaseLoader(urls)\n",
    "documents = loader.load()\n",
    "\n",
    "\n",
    "# 2. 불러온 본문을 Split (Chunking) : recursive text splitter 활용\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "# 3. Chunks 를 임베딩하여 Vector store 저장: openai, chroma 사용\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
    "\n",
    "\n",
    "# 4. User query = ‘agent memory’ 를 받아 관련된 chunks를 retrieve\n",
    "# FMMR algorithm을 사용하고 관계성이 높은 top 5 문서를 retrieve\n",
    "user_query = \"agent memory\"\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={'k': 5, 'fetch_k': 50}\n",
    ")\n",
    "retrieved_docs = retriever.invoke(user_query)\n",
    "\n",
    "\n",
    "# 5. LLM을 통해 관련성 평가\n",
    "parser = JsonOutputParser()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    User Query: {user_query}\n",
    "    Retrieved Chunk: {chunk}\n",
    "\n",
    "    Evaluate the relevance of the retrieved chunk to the user query.\n",
    "    If relevant, respond with {{\"relevance\": \"yes\"}}\n",
    "    If not relevant, respond with {{\"relevance\": \"no\"}}\n",
    "    \"\"\",\n",
    "    input_variables=[\"user_query\", \"chunk\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "\n",
    "def generate_answer(content):\n",
    "    answer_prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "            content: {content}\n",
    "            주어진 content를 받아서 100자 이내로 요약해줘.\n",
    "        \"\"\",\n",
    "        input_variables=[\"content\"]\n",
    "    )\n",
    "    answer_chain = answer_prompt | llm | parser\n",
    "    answer_chain.invoke({\"content\": content})\n",
    "    print(result)\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    chunk_content = doc.page_content\n",
    "    result = chain.invoke({\"user_query\": user_query, \"chunk\": chunk_content})\n",
    "    if result[\"relevance\"] == \"yes\":\n",
    "        generate_answer(chunk_content)\n",
    "    else:\n",
    "        print(\"No Relevance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
